{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of Contents Introduction - What is Bedrock? Security Resilience Efficiency Introduction Bedrock is a tool for provisioning infrastructure and services in a portable and consistent way. Bedrock provides Terraform-based blueprints and a collection of fine-grained roles to assist with maintaining sprawling infrastructure and services across multiple platforms. When we design modern computing architectures it is important to consider Security , Resilience and Efficiency as equally important concerns. As we evolve each of these concerns within our solutions we find that they are not mutually exclusive, but rather complement and feed into each other. This is especially true with public Cloud architectures Security Security is a concern that must be recognised and addressed throughout the entire architecture lifecycle, from inception to build to operation. It is equally important, and especially with public Cloud platfrorms, that security permeates the entire solution and is not just focused on the account perimiter. Role-based access control (RBAC) allows us to restrict actors to the minimum required permissions, which is commonly referred to as the principle of least privilege , and is the basis for the architectural blueprints provided by Bedrock. Resilience TBD. Efficiency Efficient use of resources has become a primary focus as more workloads are deployed into \"pay for what you use\" platforms such as public Cloud. However cost reduction is not the only benefit of optimized architectures, as it also feeds into other concerns such as Security and resource management. The key focus areas of Bedrock relating to efficiency include: Architectural Design - optimal architecturs for Cloud-native applications Cost Projection - cost budgeting vs actual Cost Allocation - tracking and tagging resources Compliance - pre-deployment checks and governance References Terraform Bedrock is build on top of the functionality provided by Terraform and Terraform Cloud to facilitate a simple, secure and consistent development and provisioning workflow.","title":"Introduction"},{"location":"#table-of-contents","text":"Introduction - What is Bedrock? Security Resilience Efficiency","title":"Table of Contents"},{"location":"#introduction","text":"Bedrock is a tool for provisioning infrastructure and services in a portable and consistent way. Bedrock provides Terraform-based blueprints and a collection of fine-grained roles to assist with maintaining sprawling infrastructure and services across multiple platforms. When we design modern computing architectures it is important to consider Security , Resilience and Efficiency as equally important concerns. As we evolve each of these concerns within our solutions we find that they are not mutually exclusive, but rather complement and feed into each other. This is especially true with public Cloud architectures","title":"Introduction"},{"location":"#security","text":"Security is a concern that must be recognised and addressed throughout the entire architecture lifecycle, from inception to build to operation. It is equally important, and especially with public Cloud platfrorms, that security permeates the entire solution and is not just focused on the account perimiter. Role-based access control (RBAC) allows us to restrict actors to the minimum required permissions, which is commonly referred to as the principle of least privilege , and is the basis for the architectural blueprints provided by Bedrock.","title":"Security"},{"location":"#resilience","text":"TBD.","title":"Resilience"},{"location":"#efficiency","text":"Efficient use of resources has become a primary focus as more workloads are deployed into \"pay for what you use\" platforms such as public Cloud. However cost reduction is not the only benefit of optimized architectures, as it also feeds into other concerns such as Security and resource management. The key focus areas of Bedrock relating to efficiency include: Architectural Design - optimal architecturs for Cloud-native applications Cost Projection - cost budgeting vs actual Cost Allocation - tracking and tagging resources Compliance - pre-deployment checks and governance","title":"Efficiency"},{"location":"#references","text":"","title":"References"},{"location":"#terraform","text":"Bedrock is build on top of the functionality provided by Terraform and Terraform Cloud to facilitate a simple, secure and consistent development and provisioning workflow.","title":"Terraform"},{"location":"blueprints/","text":"Bedrock Blueprints Bedrock blueprints are portable, configurable provisioning tools for Cloud infrastructure. A blueprint may be added to any automated workflow that supports Docker, to provide just-in-time provisioning of infrastructure.","title":"Overview"},{"location":"blueprints/#bedrock-blueprints","text":"Bedrock blueprints are portable, configurable provisioning tools for Cloud infrastructure. A blueprint may be added to any automated workflow that supports Docker, to provide just-in-time provisioning of infrastructure.","title":"Bedrock Blueprints"},{"location":"blueprints/BLUEPRINTS/","text":"Table of Blueprints Blueprint Description Category apachesling-server Apache Sling VM Compute apachesolr-server Apache Solr VM Compute aws-backup AWS Backup Plan Persistence aws-batch AWS Batch Job Compute aws-budgets AWS Budgets Monitoring aws-cdk AWS CDK Orchestration aws-chime AWS Chime Notifications Network aws-cloudfront AWS CloudFront Network aws-cloudwatch AWS CloudWatch Monitoring aws-codebuild AWS CodeBuild Orchestration aws-cognito AWS Cognito Security aws-config AWS Config Rules Security aws-dynamodb AWS DynamoDB Tables Persistence aws-ec2 AWS EC2 AutoScaling Compute aws-ecr AWS ECR Repositories Persistence aws-ecs AWS ECS Clusters Compute aws-iam AWS IAM Users Security aws-lambda AWS Lambda Compute aws-rds AWS RDS Databases Persistence aws-route53 AWS Route53 Zones Network aws-s3 AWS S3 Buckets Persistence aws-ses AWS SES Network aws-sns AWS SNS Topics Network aws-spotfleet AWS SpotFleet Clusters Compute aws-ssm AWS SSM Resource Groups Orchestration aws-vpc AWS VPC Subnets Network awstats-server AWStats VM Monitoring azure-container Azure Containerisation Compute bastion-aws Bastion Host for AWS Network bastion-do Bastion Host for Digital Ocean Network bastion-openstack Bastion Host for OpenStack Network cdn-endpoint CDN Endpoint Configuration Network dns-alias DNS ALIAS Record Network dns-domain DNS Domain Configuration Network dns-record DNS Record Configuration Network dns-redirect DNS Redirect Configuration Network docker-engine Docker VM Compute elasticsearch-cluster ElasticSearch Cluster Compute encryption-key PKI Encryption Key Security encryption-tls PKI TLS Certificate Security feed-subscription Manage Feed Subscriptions Application firewall-egress Firewall Egress Rules Network firewall-ingress Firewall Ingress Rules Network grafana-cloudwatch Grafana CloudWatch Integration Monitoring hashicorp-vault Hashicorp Vault VM Security kubenetes-cluster Kuberbetes Cluster Compute kubenetes-service Kuberbetes Service Compute netdata-ecs Netdata ECS Daemon Service Monitoring network-private Private Networking Network nginx-reverseproxy NGINX Reverse Proxy VM Network nginx-vhost NGINX Virtual Host Configuration Network orientdb-graph OrientDB Graph DB VM Persistence rancher-env Rancher Environment Configuration Orchestration rancher-server Rancher Server VM Compute rancher-stack Rancher Stack Configuration Orchestration squidproxy-aws Squid Proxy VM Network storage-block Block Storage (Volume) Configuration Persistence storage-bucket Bucket Storage (Object) Configuration Persistence storage-file File Storage Tools Persistence swapfile-linux Swapfile Configuration Compute","title":"Table of Blueprints"},{"location":"blueprints/BLUEPRINTS/#table-of-blueprints","text":"Blueprint Description Category apachesling-server Apache Sling VM Compute apachesolr-server Apache Solr VM Compute aws-backup AWS Backup Plan Persistence aws-batch AWS Batch Job Compute aws-budgets AWS Budgets Monitoring aws-cdk AWS CDK Orchestration aws-chime AWS Chime Notifications Network aws-cloudfront AWS CloudFront Network aws-cloudwatch AWS CloudWatch Monitoring aws-codebuild AWS CodeBuild Orchestration aws-cognito AWS Cognito Security aws-config AWS Config Rules Security aws-dynamodb AWS DynamoDB Tables Persistence aws-ec2 AWS EC2 AutoScaling Compute aws-ecr AWS ECR Repositories Persistence aws-ecs AWS ECS Clusters Compute aws-iam AWS IAM Users Security aws-lambda AWS Lambda Compute aws-rds AWS RDS Databases Persistence aws-route53 AWS Route53 Zones Network aws-s3 AWS S3 Buckets Persistence aws-ses AWS SES Network aws-sns AWS SNS Topics Network aws-spotfleet AWS SpotFleet Clusters Compute aws-ssm AWS SSM Resource Groups Orchestration aws-vpc AWS VPC Subnets Network awstats-server AWStats VM Monitoring azure-container Azure Containerisation Compute bastion-aws Bastion Host for AWS Network bastion-do Bastion Host for Digital Ocean Network bastion-openstack Bastion Host for OpenStack Network cdn-endpoint CDN Endpoint Configuration Network dns-alias DNS ALIAS Record Network dns-domain DNS Domain Configuration Network dns-record DNS Record Configuration Network dns-redirect DNS Redirect Configuration Network docker-engine Docker VM Compute elasticsearch-cluster ElasticSearch Cluster Compute encryption-key PKI Encryption Key Security encryption-tls PKI TLS Certificate Security feed-subscription Manage Feed Subscriptions Application firewall-egress Firewall Egress Rules Network firewall-ingress Firewall Ingress Rules Network grafana-cloudwatch Grafana CloudWatch Integration Monitoring hashicorp-vault Hashicorp Vault VM Security kubenetes-cluster Kuberbetes Cluster Compute kubenetes-service Kuberbetes Service Compute netdata-ecs Netdata ECS Daemon Service Monitoring network-private Private Networking Network nginx-reverseproxy NGINX Reverse Proxy VM Network nginx-vhost NGINX Virtual Host Configuration Network orientdb-graph OrientDB Graph DB VM Persistence rancher-env Rancher Environment Configuration Orchestration rancher-server Rancher Server VM Compute rancher-stack Rancher Stack Configuration Orchestration squidproxy-aws Squid Proxy VM Network storage-block Block Storage (Volume) Configuration Persistence storage-bucket Bucket Storage (Object) Configuration Persistence storage-file File Storage Tools Persistence swapfile-linux Swapfile Configuration Compute","title":"Table of Blueprints"},{"location":"blueprints/managing-state/","text":"Managing Bedrock State The Terraform state for blueprints is maintained in an AWS S3 bucket. Migrating blueprints Occasionally you may have a need to migrate existing state to a new blueprint key. The following instructions demonstrate how to do this: Ensure the old blueprint state is locally updated: $ blueprint <old_blueprint_id> [-t <old_blueprint_tag>] Rename the blueprint state on the filesystem: $ mv ~/.bedrock/<old_blueprint_id>/<old_blueprint_tag> ~/.bedrock/<new_blueprint_id>/<new_blueprint_tag> Initialise the new blueprint state using Terraform -force-copy option: $ TF_ARGS=-force-copy blueprint <new_blueprint_id> [-t <new_blueprint_tag>] NOTE: If you don't provide a blueprint tag the default tag will be the same as the blueprint id.","title":"Managing State"},{"location":"blueprints/managing-state/#managing-bedrock-state","text":"The Terraform state for blueprints is maintained in an AWS S3 bucket.","title":"Managing Bedrock State"},{"location":"blueprints/managing-state/#migrating-blueprints","text":"Occasionally you may have a need to migrate existing state to a new blueprint key. The following instructions demonstrate how to do this: Ensure the old blueprint state is locally updated: $ blueprint <old_blueprint_id> [-t <old_blueprint_tag>] Rename the blueprint state on the filesystem: $ mv ~/.bedrock/<old_blueprint_id>/<old_blueprint_tag> ~/.bedrock/<new_blueprint_id>/<new_blueprint_tag> Initialise the new blueprint state using Terraform -force-copy option: $ TF_ARGS=-force-copy blueprint <new_blueprint_id> [-t <new_blueprint_tag>] NOTE: If you don't provide a blueprint tag the default tag will be the same as the blueprint id.","title":"Migrating blueprints"},{"location":"blueprints/provision/","text":"Blueprint Provisioning Prerequisites Before provisioning a new blueprint instance you should decide which provisioning model to use. Local Local provisioning is supported by three backend types: local, S3 and remote. A local backend has no prerequisites, but is also not recommended due to potential for data loss. An S3 backend requires an AWS account configured via environment variables or local configuration file. A remote backend for local execution requires terraform cloud file or environment-based configuration. In addition local execution requires credentials for target environments, which may be the same credentials used for the backend. In fact if you require provisioning to multiple different accounts it is advisable to use a remote execution model due to limitations of local execution with multiple credentials/accounts. Remote Remote execution is supported by terraform cloud via standardised execution environments. Only the remote backend type is supported by this model. A remote backend for remote execution only requires terraform cloud credentials configured via file or environment variables. Note that there are limitations on remote execution of blueprints that contain local-exec provisioning, which also needs to be taken into consideration when deciding on the execution model. Commands Backend Configure the blueprint backend using the backend command: $ blueprint <id> backend <local|S3|remote> [options] An S3 backend requires a bucket name for state. A remote backend requires a terraform cloud organisation and a prefix for cloud workspaces. Workspace You can use workspaces to manage multiple instances of the same blueprint. This is essential when using a remote backend as the local workspace is used to define the cloud workspaces. $ blueprint <id> workspace new <name> Init After configuration of the backend and workspace, initialisation of the blueprint instance will download required plugins and modules for execution. $ blueprint <id> init Plan A blueprint plan is used to identify required input variables and preview the changes made by provisioning. If you know the required input variables you may specify on the command line. $ blueprint <id> plan [input vars] Apply A blueprint instance is provisioned using the apply command. As with the plan command input variables may be specified to override defaults. $ blueprint <id> apply [input vars] Multiple instances By default a single blueprint instance is created with the\"default\" tag. You can create multiple instances of a blueprint by using the\"tag\" option supported by all blueprint commands. $ blueprint --tag <tag> <id> <command>","title":"Provisioning"},{"location":"blueprints/provision/#blueprint-provisioning","text":"","title":"Blueprint Provisioning"},{"location":"blueprints/provision/#prerequisites","text":"Before provisioning a new blueprint instance you should decide which provisioning model to use.","title":"Prerequisites"},{"location":"blueprints/provision/#local","text":"Local provisioning is supported by three backend types: local, S3 and remote. A local backend has no prerequisites, but is also not recommended due to potential for data loss. An S3 backend requires an AWS account configured via environment variables or local configuration file. A remote backend for local execution requires terraform cloud file or environment-based configuration. In addition local execution requires credentials for target environments, which may be the same credentials used for the backend. In fact if you require provisioning to multiple different accounts it is advisable to use a remote execution model due to limitations of local execution with multiple credentials/accounts.","title":"Local"},{"location":"blueprints/provision/#remote","text":"Remote execution is supported by terraform cloud via standardised execution environments. Only the remote backend type is supported by this model. A remote backend for remote execution only requires terraform cloud credentials configured via file or environment variables. Note that there are limitations on remote execution of blueprints that contain local-exec provisioning, which also needs to be taken into consideration when deciding on the execution model.","title":"Remote"},{"location":"blueprints/provision/#commands","text":"","title":"Commands"},{"location":"blueprints/provision/#backend","text":"Configure the blueprint backend using the backend command: $ blueprint <id> backend <local|S3|remote> [options] An S3 backend requires a bucket name for state. A remote backend requires a terraform cloud organisation and a prefix for cloud workspaces.","title":"Backend"},{"location":"blueprints/provision/#workspace","text":"You can use workspaces to manage multiple instances of the same blueprint. This is essential when using a remote backend as the local workspace is used to define the cloud workspaces. $ blueprint <id> workspace new <name>","title":"Workspace"},{"location":"blueprints/provision/#init","text":"After configuration of the backend and workspace, initialisation of the blueprint instance will download required plugins and modules for execution. $ blueprint <id> init","title":"Init"},{"location":"blueprints/provision/#plan","text":"A blueprint plan is used to identify required input variables and preview the changes made by provisioning. If you know the required input variables you may specify on the command line. $ blueprint <id> plan [input vars]","title":"Plan"},{"location":"blueprints/provision/#apply","text":"A blueprint instance is provisioned using the apply command. As with the plan command input variables may be specified to override defaults. $ blueprint <id> apply [input vars]","title":"Apply"},{"location":"blueprints/provision/#multiple-instances","text":"By default a single blueprint instance is created with the\"default\" tag. You can create multiple instances of a blueprint by using the\"tag\" option supported by all blueprint commands. $ blueprint --tag <tag> <id> <command>","title":"Multiple instances"},{"location":"cli/","text":"Bedrock CLI The Bedrock command line interface (CLI) is a Python-based tool to orchestrate provisioning of blueprints and constellations.","title":"Overview"},{"location":"cli/#bedrock-cli","text":"The Bedrock command line interface (CLI) is a Python-based tool to orchestrate provisioning of blueprints and constellations.","title":"Bedrock CLI"},{"location":"console/","text":"Bedrock Console The Bedrock Console is an application suite for defining and executing orchestration pipelines for blueprints and constellations.","title":"Overview"},{"location":"console/#bedrock-console","text":"The Bedrock Console is an application suite for defining and executing orchestration pipelines for blueprints and constellations.","title":"Bedrock Console"},{"location":"patterns/ALERTING/","text":"Alerting practices This page outlines some key information on how and when alerting may be required. Overview Alerting is an important tool for responding to service degradation and constraints, however just as important is knowing when and how alerting is used. An overabundance of alerting will lead to \"alert blindness\", where all alerts are seen to be unimportant, so judicious use of alerting is just as critical as the approach.","title":"Alerting practices"},{"location":"patterns/ALERTING/#alerting-practices","text":"This page outlines some key information on how and when alerting may be required.","title":"Alerting practices"},{"location":"patterns/ALERTING/#overview","text":"Alerting is an important tool for responding to service degradation and constraints, however just as important is knowing when and how alerting is used. An overabundance of alerting will lead to \"alert blindness\", where all alerts are seen to be unimportant, so judicious use of alerting is just as critical as the approach.","title":"Overview"},{"location":"patterns/API/","text":"Publishing APIs This page provides information and reference to approaches for publishing APIs. Overview Application Programming Interfaces (APIs) provide the contract for service interoperability and integration that is a critical component of modern solution architectures. REST vs GraphQL Two common approaches to API design are HTTP Verb-based REST APIs and GraphQL, a modern client-centric approach to query composition. REST is often a good starting point for APIs, as it helps to decompose and consider how data may be isolated from one another. GraphQL in some ways can build on a good REST API by providing a mechanism for combining different data sources together on the server side and reduce chattiness of APIs. REST Verbs TBD.","title":"Publishing APIs"},{"location":"patterns/API/#publishing-apis","text":"This page provides information and reference to approaches for publishing APIs.","title":"Publishing APIs"},{"location":"patterns/API/#overview","text":"Application Programming Interfaces (APIs) provide the contract for service interoperability and integration that is a critical component of modern solution architectures.","title":"Overview"},{"location":"patterns/API/#rest-vs-graphql","text":"Two common approaches to API design are HTTP Verb-based REST APIs and GraphQL, a modern client-centric approach to query composition. REST is often a good starting point for APIs, as it helps to decompose and consider how data may be isolated from one another. GraphQL in some ways can build on a good REST API by providing a mechanism for combining different data sources together on the server side and reduce chattiness of APIs.","title":"REST vs GraphQL"},{"location":"patterns/API/#rest-verbs","text":"TBD.","title":"REST Verbs"},{"location":"patterns/AUDIT/","text":"Auditing system changes and access This page identifies functions and changes that should be audited to ensure secure service operations. Overview TBD.","title":"Auditing system changes and access"},{"location":"patterns/AUDIT/#auditing-system-changes-and-access","text":"This page identifies functions and changes that should be audited to ensure secure service operations.","title":"Auditing system changes and access"},{"location":"patterns/AUDIT/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/BACKUP/","text":"Implementing Backup Strategies This page provides information on different strategies for backup of critical data and services. Overview Every service or system is fallible, and is prone to go down at some point, so we need to ensure the recovery of a service is both quick and any data loss is minimised. A key aspect of this is a suitable backup strategy that aligns with the target service level objectives (SLOs). Recovery Point Objective (RPO) TBD.","title":"Implementing Backup Strategies"},{"location":"patterns/BACKUP/#implementing-backup-strategies","text":"This page provides information on different strategies for backup of critical data and services.","title":"Implementing Backup Strategies"},{"location":"patterns/BACKUP/#overview","text":"Every service or system is fallible, and is prone to go down at some point, so we need to ensure the recovery of a service is both quick and any data loss is minimised. A key aspect of this is a suitable backup strategy that aligns with the target service level objectives (SLOs).","title":"Overview"},{"location":"patterns/BACKUP/#recovery-point-objective-rpo","text":"TBD.","title":"Recovery Point Objective (RPO)"},{"location":"patterns/CACHE/","text":"Caching service responses This page highlights patterns and mechanism for caching data to alleviate load and improve performance. Overview Caching is an important concept to improve the availability, responsiveness and efficiency of services. Many modern architectures are a distributed collection of many microservices, which means that communication between these services can a key bottleneck of performant systems. Caching provides a way to reduce the load on services responding to requests by using precalculated responses.","title":"Caching service responses"},{"location":"patterns/CACHE/#caching-service-responses","text":"This page highlights patterns and mechanism for caching data to alleviate load and improve performance.","title":"Caching service responses"},{"location":"patterns/CACHE/#overview","text":"Caching is an important concept to improve the availability, responsiveness and efficiency of services. Many modern architectures are a distributed collection of many microservices, which means that communication between these services can a key bottleneck of performant systems. Caching provides a way to reduce the load on services responding to requests by using precalculated responses.","title":"Overview"},{"location":"patterns/CANARY/","text":"Canary Deployments This page outlines the purpose of canary deployments and why they are important. Overview A canary deployment is typically used to test service changes prior to a full cutover to the new implementation. It may be used to service a small proportion of traffic to ensure no service degradation and provide an option to revert or commit to the new deployment based on empirical testing. Blue-green vs Canary Deployments TBD.","title":"Canary Deployments"},{"location":"patterns/CANARY/#canary-deployments","text":"This page outlines the purpose of canary deployments and why they are important.","title":"Canary Deployments"},{"location":"patterns/CANARY/#overview","text":"A canary deployment is typically used to test service changes prior to a full cutover to the new implementation. It may be used to service a small proportion of traffic to ensure no service degradation and provide an option to revert or commit to the new deployment based on empirical testing.","title":"Overview"},{"location":"patterns/CANARY/#blue-green-vs-canary-deployments","text":"TBD.","title":"Blue-green vs Canary Deployments"},{"location":"patterns/DISCOVERY/","text":"Service Discovery This page provides information on Service Discovery approaches and when they should be used.","title":"Service Discovery"},{"location":"patterns/DISCOVERY/#service-discovery","text":"This page provides information on Service Discovery approaches and when they should be used.","title":"Service Discovery"},{"location":"patterns/ENCRYPT/","text":"When to use Encryption This page outlines the need for encryption in modern solution architectures. Overview Encryption of data is necessary to protect sensitive information and operation of secure services. When discussing encryption we usually think in terms of data IN-TRANSIT (e.g. data is moving from one system to another) or data AT-REST (how data is stored or persisted). Both of these concepts are crucial for secure system architectures. IN-TRANSIT vs AT-REST","title":"When to use Encryption"},{"location":"patterns/ENCRYPT/#when-to-use-encryption","text":"This page outlines the need for encryption in modern solution architectures.","title":"When to use Encryption"},{"location":"patterns/ENCRYPT/#overview","text":"Encryption of data is necessary to protect sensitive information and operation of secure services. When discussing encryption we usually think in terms of data IN-TRANSIT (e.g. data is moving from one system to another) or data AT-REST (how data is stored or persisted). Both of these concepts are crucial for secure system architectures.","title":"Overview"},{"location":"patterns/ENCRYPT/#in-transit-vs-at-rest","text":"","title":"IN-TRANSIT vs AT-REST"},{"location":"patterns/EVENT/","text":"Event-driven Architecture This page outlines some of the concepts behind event-driven architectures. Overview TBD. AWS SNS TBD. Kafka TBD.","title":"Event-driven Architecture"},{"location":"patterns/EVENT/#event-driven-architecture","text":"This page outlines some of the concepts behind event-driven architectures.","title":"Event-driven Architecture"},{"location":"patterns/EVENT/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/EVENT/#aws-sns","text":"TBD.","title":"AWS SNS"},{"location":"patterns/EVENT/#kafka","text":"TBD.","title":"Kafka"},{"location":"patterns/FIREWALL/","text":"Network Isolation This page explores best-practice regarding network isolation and service firewalls. Overview TBD.","title":"Network Isolation"},{"location":"patterns/FIREWALL/#network-isolation","text":"This page explores best-practice regarding network isolation and service firewalls.","title":"Network Isolation"},{"location":"patterns/FIREWALL/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/HEALTH/","text":"Implementing Service Health Checks This page provides patterns for how service health check may be implemented. Overiew Health checks are a critical component of responsive, highly available services. Health checks are the first line of defence when detecting and responding to service interruption, and as such need to be sophisticated enough to detect and report on anything that may be perceived as an interruption to service. TCP vs HTTP Health Checks Some platforms that use health checks to determine the state of a deployed service will provide the option for TCP or HTTP health checks. TCP health checks will simply test for an open TCP port (e.g. port 80 for web services), and assume the service is healthy. Whilst TCP health checks may be an option for non-HTTP services, as this approach alleviates the need for a dedicated health check endpoint, it is almost always not an optimal approach as there could be a number of reasons why the port is open but the service is still unhealthy. Alternatively, HTTP health checks may be used with the default service endpoint (for simple, unauthenticated services), but ideally should have a specific health check endpoint that can provide more sophisticated health reporting. Status Response Typically a health check is implemented as an HTTP endpoint that responds with appropriate HTTP status codes when invoked. Some common status codes may include: 200 - Service is healthy 503 - Service is down 504 - Backing services unreachable \u2026","title":"Implementing Service Health Checks"},{"location":"patterns/HEALTH/#implementing-service-health-checks","text":"This page provides patterns for how service health check may be implemented.","title":"Implementing Service Health Checks"},{"location":"patterns/HEALTH/#overiew","text":"Health checks are a critical component of responsive, highly available services. Health checks are the first line of defence when detecting and responding to service interruption, and as such need to be sophisticated enough to detect and report on anything that may be perceived as an interruption to service.","title":"Overiew"},{"location":"patterns/HEALTH/#tcp-vs-http-health-checks","text":"Some platforms that use health checks to determine the state of a deployed service will provide the option for TCP or HTTP health checks. TCP health checks will simply test for an open TCP port (e.g. port 80 for web services), and assume the service is healthy. Whilst TCP health checks may be an option for non-HTTP services, as this approach alleviates the need for a dedicated health check endpoint, it is almost always not an optimal approach as there could be a number of reasons why the port is open but the service is still unhealthy. Alternatively, HTTP health checks may be used with the default service endpoint (for simple, unauthenticated services), but ideally should have a specific health check endpoint that can provide more sophisticated health reporting.","title":"TCP vs HTTP Health Checks"},{"location":"patterns/HEALTH/#status-response","text":"Typically a health check is implemented as an HTTP endpoint that responds with appropriate HTTP status codes when invoked. Some common status codes may include: 200 - Service is healthy 503 - Service is down 504 - Backing services unreachable \u2026","title":"Status Response"},{"location":"patterns/IDENTITY/","text":"Authentication and Identity This page explores key concepts with user identity and access management. Overview TBD.","title":"Authentication and Identity"},{"location":"patterns/IDENTITY/#authentication-and-identity","text":"This page explores key concepts with user identity and access management.","title":"Authentication and Identity"},{"location":"patterns/IDENTITY/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/LOG/","text":"Logging practices This page provides information on best-practice for logging and things to avoid. Overview TBD.","title":"Logging practices"},{"location":"patterns/LOG/#logging-practices","text":"This page provides information on best-practice for logging and things to avoid.","title":"Logging practices"},{"location":"patterns/LOG/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/MONITOR/","text":"Service Monitoring This page outlines the reasons for monitoring and how to do it. Overview TBD.","title":"Service Monitoring"},{"location":"patterns/MONITOR/#service-monitoring","text":"This page outlines the reasons for monitoring and how to do it.","title":"Service Monitoring"},{"location":"patterns/MONITOR/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/OBSERVABILITY/","text":"Service Observability This page provides information on monitoring and observability patterns. Overview Observability is the practice of monitoring and publishing \"what is going on\" in your systems and services. This may include service performance, availability and connectivity related concerns. Monitoring TBD. Logging TBD. Distributed Tracing TBD.","title":"Service Observability"},{"location":"patterns/OBSERVABILITY/#service-observability","text":"This page provides information on monitoring and observability patterns.","title":"Service Observability"},{"location":"patterns/OBSERVABILITY/#overview","text":"Observability is the practice of monitoring and publishing \"what is going on\" in your systems and services. This may include service performance, availability and connectivity related concerns.","title":"Overview"},{"location":"patterns/OBSERVABILITY/#monitoring","text":"TBD.","title":"Monitoring"},{"location":"patterns/OBSERVABILITY/#logging","text":"TBD.","title":"Logging"},{"location":"patterns/OBSERVABILITY/#distributed-tracing","text":"TBD.","title":"Distributed Tracing"},{"location":"patterns/PROXY/","text":"The benefits of a Proxy This pages addresses some uses of a proxy component to control ingress and egress traffic for your system. Overview A proxy is essentially a network-level component that sits between your service and external integration points. This could be for traffic initiated externally (ingress) or traffic flowing out from your system (egress). Proxy vs Firewall At first glance it may seem that a proxy performs a similar function as a firewall, however they are fundamentally different. Firewalls are aimed at blocking or allowing network connectivity on specific ports, regardless of the data that flows on said ports or where it is coming from or going to. A proxy, on the other hand, is concerned with the source/destination of data, and may even inspect network packets as they are received to determine the validity of requests. As this can be a CPU intensive activity it is important to use proxying features judicously so as to not impact on system performance.","title":"The benefits of a Proxy"},{"location":"patterns/PROXY/#the-benefits-of-a-proxy","text":"This pages addresses some uses of a proxy component to control ingress and egress traffic for your system.","title":"The benefits of a Proxy"},{"location":"patterns/PROXY/#overview","text":"A proxy is essentially a network-level component that sits between your service and external integration points. This could be for traffic initiated externally (ingress) or traffic flowing out from your system (egress).","title":"Overview"},{"location":"patterns/PROXY/#proxy-vs-firewall","text":"At first glance it may seem that a proxy performs a similar function as a firewall, however they are fundamentally different. Firewalls are aimed at blocking or allowing network connectivity on specific ports, regardless of the data that flows on said ports or where it is coming from or going to. A proxy, on the other hand, is concerned with the source/destination of data, and may even inspect network packets as they are received to determine the validity of requests. As this can be a CPU intensive activity it is important to use proxying features judicously so as to not impact on system performance.","title":"Proxy vs Firewall"},{"location":"patterns/PUBSUB/","text":"Publish/Subscriber architectures This page outlines patterns for pub-sub architectures and when they make sense.","title":"Publish/Subscriber architectures"},{"location":"patterns/PUBSUB/#publishsubscriber-architectures","text":"This page outlines patterns for pub-sub architectures and when they make sense.","title":"Publish/Subscriber architectures"},{"location":"patterns/QUEUE/","text":"Queuing strategies This page provides information on queueing strategies and when to use them.","title":"Queuing strategies"},{"location":"patterns/QUEUE/#queuing-strategies","text":"This page provides information on queueing strategies and when to use them.","title":"Queuing strategies"},{"location":"patterns/RUNBOOK/","text":"Implementing Runbooks This page provides information on different types of runbook and when to use them. Overview A Runbook is essentially a series of repeatable steps in response to an event or issue identified in a system architecture. Runbooks may be manual or automated, depending on the activity being performed. Manual Runbooks A manual runbook is esentially a series of instructions for a human operator to perform. This could include things like restarting a container or virutal machine when a service is unresponsive, or isolating a component when a security breach has been detected. Automated Runbooks Ideally we would like to have entirely automated responses to system issues, and this may be something that is evolved over time. For example, after establishing a manual runbook for restarting a service, we may be able to translate this into an automated process.","title":"Implementing Runbooks"},{"location":"patterns/RUNBOOK/#implementing-runbooks","text":"This page provides information on different types of runbook and when to use them.","title":"Implementing Runbooks"},{"location":"patterns/RUNBOOK/#overview","text":"A Runbook is essentially a series of repeatable steps in response to an event or issue identified in a system architecture. Runbooks may be manual or automated, depending on the activity being performed.","title":"Overview"},{"location":"patterns/RUNBOOK/#manual-runbooks","text":"A manual runbook is esentially a series of instructions for a human operator to perform. This could include things like restarting a container or virutal machine when a service is unresponsive, or isolating a component when a security breach has been detected.","title":"Manual Runbooks"},{"location":"patterns/RUNBOOK/#automated-runbooks","text":"Ideally we would like to have entirely automated responses to system issues, and this may be something that is evolved over time. For example, after establishing a manual runbook for restarting a service, we may be able to translate this into an automated process.","title":"Automated Runbooks"},{"location":"patterns/SCALE/","text":"Horizontal Scaling This page outlines key benefits of autoscaling and how to determine right-sizing. Overview TBD.","title":"Horizontal Scaling"},{"location":"patterns/SCALE/#horizontal-scaling","text":"This page outlines key benefits of autoscaling and how to determine right-sizing.","title":"Horizontal Scaling"},{"location":"patterns/SCALE/#overview","text":"TBD.","title":"Overview"},{"location":"patterns/SECRETS/","text":"Managing Secrets This page outlines approaches for secrets management and integration with services. Overview Secrets are a critical piece of system design, and are commonly used for integrating with backing services and other systems. Somewhat counter-intuitively, we should ultimately be trying to minimise the number of secrets we manage, as the most secure secrets are no secrets. But most systems will need some secrets and the following patterns highlight approaches for maintenance. Something you know vs are You may be familiar with the multi-factor authentication model, whereby you typically provide at least two different kinds of authentication. This is often know as \"something you know\" (e.g. a password) and \"something you have\" (e.g. a temporary code from a code generator, SMS message, etc.). On some platforms, such as public Cloud infrastructure, we can use a similar concept using service roles to identify specific services (i.e. \"something you are\"). If we can use the platform roles to identify services then we can often avoid secrets (\"something you know\"), thus minimising the risk of secrets management. When you need secrets TBD.","title":"Managing Secrets"},{"location":"patterns/SECRETS/#managing-secrets","text":"This page outlines approaches for secrets management and integration with services.","title":"Managing Secrets"},{"location":"patterns/SECRETS/#overview","text":"Secrets are a critical piece of system design, and are commonly used for integrating with backing services and other systems. Somewhat counter-intuitively, we should ultimately be trying to minimise the number of secrets we manage, as the most secure secrets are no secrets. But most systems will need some secrets and the following patterns highlight approaches for maintenance.","title":"Overview"},{"location":"patterns/SECRETS/#something-you-know-vs-are","text":"You may be familiar with the multi-factor authentication model, whereby you typically provide at least two different kinds of authentication. This is often know as \"something you know\" (e.g. a password) and \"something you have\" (e.g. a temporary code from a code generator, SMS message, etc.). On some platforms, such as public Cloud infrastructure, we can use a similar concept using service roles to identify specific services (i.e. \"something you are\"). If we can use the platform roles to identify services then we can often avoid secrets (\"something you know\"), thus minimising the risk of secrets management.","title":"Something you know vs are"},{"location":"patterns/SECRETS/#when-you-need-secrets","text":"TBD.","title":"When you need secrets"},{"location":"patterns/SERVERLESS/","text":"Serverless Architectures This page provides information on the ways to adopt serverless architectures and minimise the number of self-managed resources. Overview Serverless computing is essentially using managed infrastructure to deploy services and resources. This may include compute workloads, data persistence and integration tooling that may traditionally have been self-hosted on virtual machines. Compute Traditionally compute resources have required virtual machines (VMs) for hosting software applications that provide a service. Increasingly there are more serverless options for hosting and running such applications without VMs. Platforms offerings such as Container-as-a-service (CaaS) and Function-as-a-service (FaaS) provide the ability to deploy varied compute resources without a need to maintain the underlying VM infrastructure. Resources Often static resources don't need a dedicated compute resource to deliver as they are simply files that require delivery to the requestor. As such we can often use file storage services and content delivery networks (CDNs) to serve static resources without any intermediate compute resources. Data Serverless databases are now quite common in both traditional Relational and NoSQL variants.","title":"Serverless Architectures"},{"location":"patterns/SERVERLESS/#serverless-architectures","text":"This page provides information on the ways to adopt serverless architectures and minimise the number of self-managed resources.","title":"Serverless Architectures"},{"location":"patterns/SERVERLESS/#overview","text":"Serverless computing is essentially using managed infrastructure to deploy services and resources. This may include compute workloads, data persistence and integration tooling that may traditionally have been self-hosted on virtual machines.","title":"Overview"},{"location":"patterns/SERVERLESS/#compute","text":"Traditionally compute resources have required virtual machines (VMs) for hosting software applications that provide a service. Increasingly there are more serverless options for hosting and running such applications without VMs. Platforms offerings such as Container-as-a-service (CaaS) and Function-as-a-service (FaaS) provide the ability to deploy varied compute resources without a need to maintain the underlying VM infrastructure.","title":"Compute"},{"location":"patterns/SERVERLESS/#resources","text":"Often static resources don't need a dedicated compute resource to deliver as they are simply files that require delivery to the requestor. As such we can often use file storage services and content delivery networks (CDNs) to serve static resources without any intermediate compute resources.","title":"Resources"},{"location":"patterns/SERVERLESS/#data","text":"Serverless databases are now quite common in both traditional Relational and NoSQL variants.","title":"Data"},{"location":"patterns/TRACE/","text":"Distributed Tracing This page provides information on how to implement distributed tracing. Overview TBD.","title":"Distributed Tracing"},{"location":"patterns/TRACE/#distributed-tracing","text":"This page provides information on how to implement distributed tracing.","title":"Distributed Tracing"},{"location":"patterns/TRACE/#overview","text":"TBD.","title":"Overview"},{"location":"roles/","text":"Bedrock Roles Bedrock Roles define a clearly-defined collection of fine-grained roles and permissions for provisioning blueprints. In addition to the credentials required to access a cloud environment we can optionally specify additional roles to assume when provisioning blueprints. The benefit of such fine-grained roles is that it provides additional security to ensure a blueprint can make changes outside the scope of the blueprint specification.","title":"Bedrock Roles"},{"location":"roles/#bedrock-roles","text":"Bedrock Roles define a clearly-defined collection of fine-grained roles and permissions for provisioning blueprints. In addition to the credentials required to access a cloud environment we can optionally specify additional roles to assume when provisioning blueprints. The benefit of such fine-grained roles is that it provides additional security to ensure a blueprint can make changes outside the scope of the blueprint specification.","title":"Bedrock Roles"},{"location":"roles/ROLES/","text":"Table of Roles and Permissions Roles Description Permissions apachesling-aws Managagement of Apache Sling on AWS Cloudformation, EC2, CloudWatch Logs, Route53 apachesolr-aws Managagement of Apache Solr on AWS Cloudformation, EC2, CloudWatch Logs, Route53 aws-chime AWS Chime Notifications Lambda, CloudWatch Logs aws-cloudfront AWS CloudFront Management Lambda, Lamda@Edge, CloudFront, S3, Route53, CloudWatch Logs aws-cloudwatch AWS CloudWatch Events Management CloudWatch Events, Lambda aws-codebuild AWS CodeBuild Management CodeBuild, KMS, SSM Parameter Store, EC2, CloudWatch Logs aws-dynamodb AWS DynamoDB Management DynamoDB, Lambda, CloudWatch Logs Custom Policies Description Permissions cloudformation-create AWS CloudFormation Management CloudFormation","title":"Roles and Permissions"},{"location":"roles/ROLES/#table-of-roles-and-permissions","text":"Roles Description Permissions apachesling-aws Managagement of Apache Sling on AWS Cloudformation, EC2, CloudWatch Logs, Route53 apachesolr-aws Managagement of Apache Solr on AWS Cloudformation, EC2, CloudWatch Logs, Route53 aws-chime AWS Chime Notifications Lambda, CloudWatch Logs aws-cloudfront AWS CloudFront Management Lambda, Lamda@Edge, CloudFront, S3, Route53, CloudWatch Logs aws-cloudwatch AWS CloudWatch Events Management CloudWatch Events, Lambda aws-codebuild AWS CodeBuild Management CodeBuild, KMS, SSM Parameter Store, EC2, CloudWatch Logs aws-dynamodb AWS DynamoDB Management DynamoDB, Lambda, CloudWatch Logs Custom Policies Description Permissions cloudformation-create AWS CloudFormation Management CloudFormation","title":"Table of Roles and Permissions"}]}